{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programmers\\envs\\vit\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.4.1+cu118\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/pytorch\")\n",
    "from segmentation_models_pytorch.utils.imports import *\n",
    "\n",
    "print_versions()\n",
    "\n",
    "# Select device (GPU or CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 534, gt: 534\n",
      "Number of validation images: 150, gt: 150\n",
      "Number of test images: 138, gt: 138\n",
      "--------------------------------------------------\n",
      "Class Weights: {np.float32(0.0): np.float64(1.0), np.float32(1.0): np.float64(3.9)}\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory for your dataset\n",
    "DATASET_DIR = \"VH\"\n",
    "img_sub, msk_sub = 'img', 'gt'\n",
    "\n",
    "# Load paths for training, validation, and test sets with default subdirectories\n",
    "train_imgs, train_masks = get_dataset_paths(DATASET_DIR, split='train', img_subdir=img_sub, mask_subdir=msk_sub, mask_ext='tiff')\n",
    "val_imgs, val_masks = get_dataset_paths(DATASET_DIR, split='val', img_subdir=img_sub, mask_subdir=msk_sub, mask_ext='tiff')\n",
    "test_imgs, test_masks = get_dataset_paths(DATASET_DIR, split='test', img_subdir=img_sub, mask_subdir=msk_sub, mask_ext='tiff')\n",
    "\n",
    "# Verify images\n",
    "print(f\"Number of training images: {len(train_imgs)}, gt: {len(train_masks)}\")\n",
    "print(f\"Number of validation images: {len(val_imgs)}, gt: {len(val_masks)}\")\n",
    "print(f\"Number of test images: {len(test_imgs)}, gt: {len(test_masks)}\")\n",
    "print(50*'-')\n",
    "\n",
    "weights = calculate_class_weights(train_masks)\n",
    "print(\"Class Weights:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip()\n",
    "])\n",
    "\n",
    "# Train dataset with the same transformations for both images and masks\n",
    "train_dataset = Dataset2D(train_imgs, train_masks, transform=transform, transform_label=transform)\n",
    "\n",
    "# Validation and test datasets only convert to tensors\n",
    "val_dataset = Dataset2D(val_imgs, val_masks, transform=transforms.ToTensor(), transform_label=None)\n",
    "test_dataset = Dataset2D(test_imgs, test_masks, transform=transforms.ToTensor(), transform_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNetEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.input_dim + self.hidden_dim,\n",
    "            out_channels=4 * self.hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        combined = torch.cat([x, h_prev], dim=1)  # Concatenate along channel axis\n",
    "        conv_output = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_dim, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c = f * c_prev + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            ConvLSTMCell(\n",
    "                input_dim=input_dim if i == 0 else hidden_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                kernel_size=kernel_size\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C, H, W]\n",
    "        B, T, C, H, W = x.shape\n",
    "        h, c = self.init_hidden(B, C, H, W)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            inp = x[:, t, :, :, :]\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                h[i], c[i] = layer(inp, (h[i], c[i]))\n",
    "                inp = h[i]\n",
    "            outputs.append(h[-1])\n",
    "\n",
    "        return torch.stack(outputs, dim=1), (h, c)\n",
    "\n",
    "    def init_hidden(self, B, C, H, W):\n",
    "        h = [torch.zeros(B, C, H, W, device=next(self.parameters()).device) for _ in range(self.num_layers)]\n",
    "        c = [torch.zeros(B, C, H, W, device=next(self.parameters()).device) for _ in range(self.num_layers)]\n",
    "        return h, c\n",
    "\n",
    "class CNNConvLSTMNet(nn.Module):\n",
    "    def __init__(self, cnn_backbone, feature_channels, temporal_channels, kernel_size, num_layers):\n",
    "        super(CNNConvLSTMNet, self).__init__()\n",
    "\n",
    "        # CNN Backbone (e.g., U-Net encoder or other feature extractor)\n",
    "        self.cnn_backbone = cnn_backbone\n",
    "\n",
    "        # ConvLSTM for temporal processing\n",
    "        self.conv_lstm = ConvLSTM(\n",
    "            input_dim=feature_channels,\n",
    "            hidden_dim=temporal_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Decoder for segmentation (basic example; replaceable by U-Net decoder)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(temporal_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input tensor of shape [B, T, H, W]\n",
    "        \"\"\"\n",
    "        B, T, H, W = x.shape\n",
    "\n",
    "        # Step 1: Extract CNN features for each time frame\n",
    "        cnn_features = []\n",
    "        for t in range(T):\n",
    "            frame = x[:, t, :, :].unsqueeze(1)  # Extract frame [B, 1, H, W]\n",
    "            cnn_features.append(self.cnn_backbone(frame))  # Shape [B, C, H', W']\n",
    "        \n",
    "        cnn_features = torch.stack(cnn_features, dim=1)  # Shape [B, T, C, H', W']\n",
    "\n",
    "        # Step 2: Process features with ConvLSTM\n",
    "        conv_lstm_out, _ = self.conv_lstm(cnn_features)  # Shape [B, T, temporal_channels, H', W']\n",
    "\n",
    "        # Step 3: Decode the last ConvLSTM output\n",
    "        last_output = conv_lstm_out[:, -1, :, :, :]  # Use the last time step [B, temporal_channels, H', W']\n",
    "        segmentation_output = self.decoder(last_output)  # [B, 1, H, W]\n",
    "        \n",
    "        return segmentation_output.squeeze(1)  # [B, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n",
    "\n",
    "class EfficientNetB7Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, output_channels=32):\n",
    "        super(EfficientNetB7Backbone, self).__init__()\n",
    "        if pretrained:\n",
    "            self.model = efficientnet_b7(weights=EfficientNet_B7_Weights.DEFAULT)\n",
    "        else:\n",
    "            self.model = efficientnet_b7(weights=None)\n",
    "\n",
    "        self.features = self.model.features\n",
    "        # Update the input channels of reduce_channels to 2560\n",
    "        self.reduce_channels = nn.Conv2d(2560, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.expand(-1, 3, -1, -1)  # Expand input to 3 channels\n",
    "        features = self.features(x)  # Extract features from EfficientNet-B7\n",
    "        reduced_features = self.reduce_channels(features)  # Reduce the number of channels\n",
    "        return reduced_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace DummyCNN with UNetEncoder\n",
    "# cnn_backbone = UNetEncoder()\n",
    "cnn_backbone = EfficientNetB7Backbone(pretrained=True)\n",
    "\n",
    "\n",
    "# Initialize the CNN-ConvLSTM model\n",
    "model = CNNConvLSTMNet(cnn_backbone, feature_channels=32, temporal_channels=32, kernel_size=3, num_layers=1)\n",
    "\n",
    "# Example input: [B, T, H, W]\n",
    "input_tensor = torch.randn(5, 11, 512, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "print(\"Output shape:\", output.shape)  # Expected: [B, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "58165sdfvsdfvssdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "ENCODER = 'efficientnet-b7' #resnet101 #efficientnet-b7 #resnext101_32x16d\n",
    "ENCODER_WEIGHTS = 'imagenet' #'instagram'\n",
    "CLASSES = ['1']  # For binary segmentation, only one class is needed\n",
    "ACTIVATION = 'sigmoid' #'softmax' #None #'softmax' #'sigmoid' if len(CLASSES) == 1 else 'softmax'\n",
    "\n",
    "# Initialize the U-Net model with the specified encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION,\n",
    "    in_channels=11\n",
    "    #decoder_atrous_rates=(1,2,4) # DLV3PLus\n",
    ")\n",
    "\n",
    "# Get the preprocessing function for the chosen encoder\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE_TRAIN = 5\n",
    "BATCH_SIZE_VALID = 10\n",
    "BATCH_SIZE_TEST = 10\n",
    "\n",
    "# Define the loss function (DiceLoss or CrossEntropyLoss)\n",
    "loss = smp.utils.losses.DiceLoss()  # Change to CrossEntropyLoss() if needed\n",
    "#loss = smp.utils.losses.CrossEntropyLoss()\n",
    "\n",
    "# Define the metric for evaluation. IoU (Intersection over Union) is a standard metric for segmentation.\n",
    "#metrics = [smp.utils.metrics.mIoU()]\n",
    "metrics = [smp.utils.metrics.IoU()]\n",
    "\n",
    "# Initialize the optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE_VALID, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False, num_workers=0)\n",
    "\n",
    "# Training loop setup using SMP utilities\n",
    "train_epoch = TrainEpoch(model, loss=loss, metrics=metrics, optimizer=opt, device=DEVICE)\n",
    "valid_epoch = ValidEpoch(model, loss=loss, metrics=metrics, device=DEVICE)\n",
    "\n",
    "# Verify batch\n",
    "for images, labels in train_loader:\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the minimum dice loss and max IoU for saving the best model\n",
    "max_iou = 0\n",
    "\n",
    "# Number of epochs to train\n",
    "EPOCHS = 200\n",
    "\n",
    "# Model save path\n",
    "model_save_dir = 'test_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Run the training loop for the specified number of epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nEpoch: {epoch + 1}/{EPOCHS}')\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    # If validation IoU improves, save the model's state dictionary\n",
    "    if max_iou < valid_logs['iou_score']:\n",
    "        max_iou = valid_logs['iou_score']\n",
    "        torch.save(model.state_dict(), 'test_models/test_convlstm_b7.pth')\n",
    "        print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max IoU: {max_iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('test_models/test_convlstm_b7.pth'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.utils.model_eval import display_binary_metrics\n",
    "\n",
    "metrics_df = display_binary_metrics(model, test_loader, DEVICE, threshold=0.5, \n",
    "                               show_iou=True, show_precision=True, show_recall=True, show_f1_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.utils.visualization import visualize_predictions\n",
    "\n",
    "# For binary segmentation:\n",
    "visualize_predictions(model, valid_loader, DEVICE, num_images=5, binary=True, threshold=0.5)\n",
    "\n",
    "# For multiclass segmentation:\n",
    "# visualize_predictions(model, valid_loader, DEVICE, num_images=5, binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
